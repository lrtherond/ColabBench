{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuOSsGNDHOVl"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install bitsandbytes\n",
        "!pip install datasets\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub\n",
        "\n",
        "!git config --global credential.helper store\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "0AqYEuMNJFG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import bitsandbytes as bnb\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from functools import partial\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    AutoPeftModelForCausalLM,\n",
        ")\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    set_seed,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n"
      ],
      "metadata": {
        "id": "5DgKeQF9JeyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, bnb_config):\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    max_memory = f'{40960}MB'\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",  # dispatch efficiently the model on the available resources\n",
        "        max_memory={i: max_memory for i in range(n_gpus)},\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True, add_eos_token=True)\n",
        "    print(repr(tokenizer.pad_token))\n",
        "    print(repr(tokenizer.bos_token))\n",
        "    print(repr(tokenizer.eos_token))\n",
        "\n",
        "    # Needed for LLaMA tokenizer: This code doesn't pad, but...\n",
        "    # tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = 18610\n",
        "\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "G4oRsTyGJ_iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import re\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('lrtherond/running-qa')"
      ],
      "metadata": {
        "id": "zKiaUudSNAuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset['train'].shuffle(seed=42)\n",
        "\n",
        "import re\n",
        "\n",
        "B_S = \"<s>\"\n",
        "E_S = \"</s>\"\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "\n",
        "SYSTEM_MESSAGE = \"You are a helpful, respectful, and honest coach. You help runners of all levels, from beginners to Olympians. You always respond in the style of Hal Higdon. Your answers are self-sufficient and do not reference any other resources. If a question does not make sense or is not factually coherent, explain why instead of answering incorrectly. If you don't know the answer to a question, please don't share false information.\"\n",
        "\n",
        "def format_prompt(sample):\n",
        "    \"\"\"Transform a (question, answer) pair into NLI format.\"\"\"\n",
        "\n",
        "    question = sample['question']\n",
        "    answer = sample['answer']\n",
        "\n",
        "    # Replace the question and answer with the new template\n",
        "    text = f\"\"\"{B_INST} {B_SYS}{SYSTEM_MESSAGE}{E_SYS}{question.strip()} {E_INST} {answer.strip()}\"\"\"\n",
        "\n",
        "    sample[\"text\"] = text\n",
        "\n",
        "    return sample\n"
      ],
      "metadata": {
        "id": "YZIrsfYENkFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of prompts: {len(dataset)}')\n",
        "print(f'Column names are: {dataset.column_names}')\n"
      ],
      "metadata": {
        "id": "hXegF7CON0bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_length(model):\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max length: {max_length}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "\n",
        "    return max_length\n",
        "\n",
        "\n",
        "def preprocess_batch(batch, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Tokenizing a batch\n",
        "    \"\"\"\n",
        "\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
        "    \"\"\"Format & tokenize it so it is ready for training\n",
        "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
        "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    # Add prompt to each sample\n",
        "    print(\"Preprocessing dataset...\")\n",
        "    dataset = dataset.map(format_prompt)\n",
        "\n",
        "    # Apply preprocessing to each batch of the dataset\n",
        "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
        "    dataset = dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched=True,\n",
        "        remove_columns=[\"question\", \"answer\", \"text\"],\n",
        "    )\n",
        "\n",
        "    # Filter out samples that have input_ids exceeding max_length\n",
        "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
        "\n",
        "    # Shuffle dataset\n",
        "    dataset = dataset.shuffle(seed=seed)\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "J5uJ1z6RN-qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bnb_config():\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "\n",
        "    return bnb_config\n"
      ],
      "metadata": {
        "id": "Ja5bqAzDRQOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_peft_config(modules):\n",
        "    \"\"\"\n",
        "    Create Parameter-Efficient Fine-Tuning config for your model\n",
        "    :param modules: Names of the modules to apply Lora to\n",
        "    \"\"\"\n",
        "    config = LoraConfig(\n",
        "        r=16,                # dimension of the updated matrices\n",
        "        lora_alpha=64,       # parameter for scaling\n",
        "        target_modules=modules,\n",
        "        lora_dropout=0.1,    # dropout probability for layers\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    return config\n"
      ],
      "metadata": {
        "id": "QyIEF6tIS8Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_all_linear_names(model):\n",
        "    cls = bnb.nn.Linear4bit\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "\n",
        "    return list(lora_module_names)\n"
      ],
      "metadata": {
        "id": "6V4-S1egTB6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model, use_4bit=False):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        num_params = param.numel()\n",
        "\n",
        "        # if using DS Zero 3 and the weights are initialized empty\n",
        "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
        "            num_params = param.ds_numel\n",
        "\n",
        "        all_param += num_params\n",
        "        if param.requires_grad:\n",
        "            trainable_params += num_params\n",
        "\n",
        "    if use_4bit:\n",
        "        trainable_params /= 2\n",
        "\n",
        "    print(\n",
        "        f\"all params: {all_param:,d} || \"\n",
        "        f\"trainable params: {trainable_params:,d} || \"\n",
        "        f\"trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "ggrqhhkoTUul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a model from Hugging Face with user's token and with bitsandbytes config\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "bnb_config = create_bnb_config()\n",
        "model, tokenizer = load_model(model_name, bnb_config)\n"
      ],
      "metadata": {
        "id": "jy6SdEF3TmgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess dataset\n",
        "\n",
        "max_length = get_max_length(model)\n",
        "seed = 42\n",
        "\n",
        "dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)"
      ],
      "metadata": {
        "id": "lTSyyEvWbqBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset\n"
      ],
      "metadata": {
        "id": "CrEuBYSKb-q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, tokenizer, dataset, output_dir):\n",
        "    # Enable gradient checkpointing to reduce memory usage\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    # Prepare the model for k-bit training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Get the names of Lora modules and create PEFT config\n",
        "    modules = find_all_linear_names(model)\n",
        "    peft_config = create_peft_config(modules)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Print information about trainable parameters\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    # Define training parameters\n",
        "    training_args = TrainingArguments(\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=2,\n",
        "        # max_steps=20, ** Can be used instead of num_train_epochs\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "    )\n",
        "\n",
        "    # Create the trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset,\n",
        "        args=training_args,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "    )\n",
        "\n",
        "    # Disable caching during training\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    # Verify data types\n",
        "    # ...\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Training...\")\n",
        "\n",
        "    train_result = trainer.train()\n",
        "\n",
        "    metrics = train_result.metrics\n",
        "    trainer.log_metrics(\"train\", metrics)\n",
        "    trainer.save_metrics(\"train\", metrics)\n",
        "    trainer.save_state()\n",
        "    print(metrics)\n",
        "\n",
        "    # Save the trained model\n",
        "    print(\"Saving last checkpoint of the model...\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    trainer.model.save_pretrained(output_dir)\n",
        "\n",
        "    # Free GPU memory\n",
        "    del model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Define output directory and call the training function\n",
        "output_dir = \"results/Llama-2-7b-chat-hf-running-qa\"\n",
        "\n",
        "train(model, tokenizer, dataset, output_dir)\n"
      ],
      "metadata": {
        "id": "5R6o0IQrULJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s7ObfYNfjts8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r outputs drive/MyDrive/Llama-2-7b-chat-hf-running-qa\n",
        "#!cp -r results drive/MyDrive/Llama-2-7b-chat-hf-running-qa"
      ],
      "metadata": {
        "id": "WTi2bPqijvxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r drive/MyDrive/Llama-2-7b-chat-hf-running-qa/outputs .\n",
        "#!cp -r drive/MyDrive/Llama-2-7b-chat-hf-running-qa/results ."
      ],
      "metadata": {
        "id": "qO94iPb8nPTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained model with specified parameters\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    output_dir,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Merge and unload the model\n",
        "model = model.merge_and_unload()\n"
      ],
      "metadata": {
        "id": "5wn1rA--XFAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the output directory for the merged checkpoint\n",
        "output_merged_dir = \"results/Llama-2-7b-chat-hf-running-qa/final_merged_checkpoint\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(output_merged_dir, exist_ok=True)\n",
        "\n",
        "# Save the model in the defined directory with safe serialization\n",
        "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "ODEJHNnmi5iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer associated with the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Save the tokenizer in the same directory as the model\n",
        "tokenizer.save_pretrained(output_merged_dir)"
      ],
      "metadata": {
        "id": "kijj5SXWqykb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"lrtherond/Llama-2-7b-chat-hf-running-qa\")"
      ],
      "metadata": {
        "id": "2bHCwE5XX11E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\"lrtherond/Llama-2-7b-chat-hf-running-qa\")"
      ],
      "metadata": {
        "id": "sRwAJ7NVZbsQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}